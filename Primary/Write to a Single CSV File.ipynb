{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf6053c0-3bee-4954-a750-705089119963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Using Spark to Write Data to a Single CSV File\n",
    "\n",
    "Apache Spark is a system designed to work with very large datasets.  Its default behavior reflects the assumption that you will be working with a large dataset that is split across\n",
    "many nodes in a cluster.\n",
    "\n",
    "When you use Apache Spark to write a dataframe to disk, you will notice that it writes the data into multiple files.  Let's look at an example and see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6afb9606-cf32-42df-ab2a-8dc672485fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, we just read in some sample data so we have a Spark dataframe\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"dbfs:/databricks-datasets/atlas_higgs/atlas_higgs.csv\")\n",
    "\n",
    "# Now, let's write this data out in CSV format so we can see how Spark writes the files\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"/my-output/default-csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57a37efd-a63d-46db-ad25-6fd829df6160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's take a look at the CSV files that Spark wrote..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cba8e19f-3d6c-4cc6-b1f2-fdc0b75c9ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/my-output/default-csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "244efb44-d4db-4d10-86a9-5d8549c64504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You will notice that our dataset was not written to one, single CSV file in a nice, tidy format.  Instead, the rows are spread out across a bunch of different CSV files.  Spark can\n",
    "work easily with these multiple files.  However, if you want to share this data with other systems, having multiple files can be cumbersome.\n",
    "\n",
    "Before we look at how to change Spark's behavior, we need to understand *why* Spark writes the data this way.\n",
    "\n",
    "The key thing to always remember about Spark is that the data is always spread out across multiple computers.  The data doesn't reside in the memory of just one computer.  It has\n",
    "been divided into multiple partitions, and those partitions are distributed across many computers.\n",
    "\n",
    "When you tell Spark to write your data, it completes this operation in parallel.  The driver tells all of the nodes to start writing their data *at the same time*.  So each node in\n",
    "the cluster starts writing all of the partitions that it has at the same time all of the other nodes are writing all of their partitions.  Therefore, Spark can't write the data to just\n",
    "one file because all of the nodes would be tripping over each other.  They would each try to write to the same file and end up overwriting the data that other nodes had written.\n",
    "\n",
    "To solve this problem, Spark saves the data from each partition to its own file.  Therefore, the number of files that get written is equal to the number of partitions that Spark\n",
    "created for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e4338ec-e3b1-4d20-9a01-6c2db7bdc516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Changing Spark's Behavior\n",
    "\n",
    "While Spark is designed to work with large, mult-terabyte datasets that could never fit into the memory of just one computer, we sometimes use it to work with smaller datasets.\n",
    "And sometime this dataset is relatively small... just a couple of gigabytes or even a few hundred megabytes. ***If*** you find yourself working with a small dataset like this, you\n",
    "can get Spark to write the data to just one file.\n",
    "\n",
    "That last point is very important and bears repeating.  To make this work, all of the data must be loaded into the memory of just one computer.  Therefore, this technique only works\n",
    "on small datasets.  If the nodes in your cluster each have 16GB of RAM, then you can probably make this work with 10GB of data or less.  If you have a dataset that is bigger than\n",
    "the amount of RAM on each node, you cannot use this technique because you will risk crashing your cluster.\n",
    "\n",
    "Fortunately, our sample dataset above is less than 100MB.  So, keeping in mind the important limitation described above, this dataset should easily fit in the memory of just one PC.\n",
    "So let's proceed with writing out our dataset to just one CSV file.  There are a couple of ways to achieve this, and we will look at both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b49b859-fe99-4617-8d54-db7760ba7b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 1:  Use the `coalesce` Feature\n",
    "The Spark Dataframe API has a method called [coalesce](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce) that tells Spark to shuffle\n",
    "your data into the specified number of partitions.  Since our dataset is small, we use this to tell Spark to rearrange our data into a single partition before writing out the\n",
    "data.\n",
    "\n",
    "Note, though, that there is a performance penalty for this.  Before writing the data, Spark must shuffle the data from all of the nodes to a single partition on a single node.  This\n",
    "takes time and puts traffic on the cluster's network.  For a ver small dataset (like the one here in our example), this is a small penalty, but it will increase as the size of your\n",
    "data increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6efa6ea-b19a-4858-a7f3-16b6b7fbe3b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df\\\n",
    ".coalesce(1)\\\n",
    ".write\\\n",
    ".format(\"csv\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(\"/my-output/coalesce-csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09812e9a-c0c7-4334-b037-6331d5891551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's take a look at the files created by Spark after using the `coalesce` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72589ebb-5fe5-47c6-b266-cb93aff0cc59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/my-output/coalesce-csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e711f0ac-8058-4809-b1c5-bcdb00dc0856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You will notice that Spark still wrote the data into a directory, and that directory has multiple files.  There are the Spark control files (e.g. the \"SUCCESS\" file,\n",
    "the \"started\" file, and the \"committed\" file).  But there is only Cone SV file containing our data.  Unfortunately, this file does not have a friendly name.  If we want to share\n",
    "this file, we may want to rename it to something shorter.  We can Python to clean up the control files and rename the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "602d81cb-ec41-45d2-af1c-29c54fde24fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_location = \"/my-output/coalesce-csv/\"\n",
    "\n",
    "files = dbutils.fs.ls(data_location)\n",
    "csv_file = [x.path for x in files if x.path.endswith(\".csv\")][0]\n",
    "dbutils.fs.mv(csv_file, data_location.rstrip('/') + \".csv\")\n",
    "dbutils.fs.rm(data_location, recurse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3001e8f9-7d32-44a4-ae47-d8e9bab1a1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's take one more look at our files to see that we have just one CSV file with a nice, friendly name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b2d02c-41a2-408f-bd08-7f93f0eb53e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/my-output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5febbb70-cc36-4c8b-97bb-67b94f73c50e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Option 2: Use `collect` and Pandas\n",
    "\n",
    "If you've used Python for data science work, you may be familiar with the `pandas` package.  This popular tool allows you to create in-memory dataframes on a single computer.\n",
    "If your Spark dataframe is small enough to fit into the RAM of your cluster's driver node, then you can simply convert your Spark dataframe to a pandas dataframe.  Then you\n",
    "can use the standard `pandas` functionality to save your pandas dataframe to a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08461d67-3fc1-4088-80a5-263bc894d666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd = df.toPandas()\n",
    "pd.to_csv(\"/dbfs/my-output/pandas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8c96544-9b9f-460d-a26a-20fe3ed855ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And now if we look at our output directory, we will see our new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd1bf168-3ab7-41e1-9aa7-79b274469677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/my-output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b6743b4-0d5a-43df-8d74-3e328626d85e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "That was super easy!  But you must be very careful with this approach.  It will *only* work with small datasets.  If you try to convert a large dataframe to a pandas dataframe, you could\n",
    "crash the driver node of your cluster.  Make sure your driver node has enough RAM to hold the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b6b0bc3-31c7-46a9-9fb0-281a8c7f9729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "One other note on this approach.  You will notice that throughout this notebook we have written data to the DBFS.  We've done this using paths relative to the root of the DBFS, like:\n",
    "`/my-output/coalesce-csv`.  In Databricks, Spark and the `dbutils` tool are all \"DBFS-aware\".  Whenever you supply a filepath to these tools, it assumes that you want to use the DBFS.\n",
    "Non-Spark tools (like the `pandas` tool) are *not* \"DBFS-aware\".  Whenever you give them a filepath, they assume you want to use the filesystem of the driver node.  Therefore, you must\n",
    "add `/dbfs/` to the beginning of your filepath so these tools will look in the right place.  For example, when we used the `to_csv` method from the `pandas` package, we had to use\n",
    "`/dbfs/my-output/pandas.csv` as our location."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Write to a Single CSV File",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
